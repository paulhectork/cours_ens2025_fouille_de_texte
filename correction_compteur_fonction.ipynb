{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bcda05-c2aa-412a-9d40-ea7dfe6a6fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longueur avant nettoyage: 63436\n",
      "longueur après nettoyage: 40061\n",
      "nombre de mots uniques: 8081\n",
      "mots les plus fréquemment utilisés: ['had']\n",
      "nombre moyen d'occurrences d'un mot:  4.9574310110134885\n",
      "\n",
      "longueur avant nettoyage: 78141\n",
      "longueur après nettoyage: 47935\n",
      "nombre de mots uniques: 9551\n",
      "mots les plus fréquemment utilisés: ['are']\n",
      "nombre moyen d'occurrences d'un mot:  5.0188461941158\n",
      "\n",
      "longueur avant nettoyage: 70004\n",
      "longueur après nettoyage: 42044\n",
      "nombre de mots uniques: 7289\n",
      "mots les plus fréquemment utilisés: ['had']\n",
      "nombre moyen d'occurrences d'un mot:  5.7681437782960625\n"
     ]
    }
   ],
   "source": [
    "with open(\"in_mrs_dalloway.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    mrs_dalloway = f.read()\n",
    "with open(\"in_the_waves.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    the_waves = f.read()\n",
    "with open(\"in_to_the_lighthouse.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    lighthouse = f.read()\n",
    "\n",
    "# intégrer le code du dessus à une fonction nommée \"count_words\" qui prenne en argument une chaîne de caractère\n",
    "# (l'un des trois textes ci-dessus) et affiche des statistiques dessus.\n",
    "def count_words(corpus_contenu): \n",
    "    # nettoyer corpus_contenu\n",
    "    corpus_contenu = corpus_contenu.lower()\n",
    "    \n",
    "    # quelques signes de ponctuation communs dans le texte à supprimer. \n",
    "    # il y en a surement d'autres, mais bon avec ça on se sera déjà débasarré de la majorité. \n",
    "    caracteres_a_supprimer = [\".\", \",\", \";\", \":\", \"?\", \"!\", \"•\", \"[\", \"]\", \"(\", \")\", \"«\", \"»\", \"*\", \"_\", \"–\"]\n",
    "    # qu'est-ce que je fais ici ?\n",
    "    for caractere in caracteres_a_supprimer:\n",
    "        corpus_contenu = corpus_contenu.replace(caractere, \"\")\n",
    "    \n",
    "    corpus_mots_full = corpus_contenu.split()  # tous les mots du corpus\n",
    "    corpus_mots = [] # ici, on ne mettra que les mots qui \n",
    "    \n",
    "    # le corpus est en français et en anglais ! on met donc des mots des deux langues\n",
    "    mots_a_supprimer = [ \"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \n",
    "                         \"mr\", \"mme\", \"mon\", \"ma\", \"mes\", \"notre\", \"votre\", \"son\", \"sa\", \"des\",\n",
    "                         \"avec\", \"les\", \"aux\", \"est\", \"que\", \"plus\", \"tout\", \"pour\",\n",
    "                         \"and\", \"for\", \"with\", \"mine\", \"yours\", \"his\", \"hers\", \"our\", \"ours\",\n",
    "                         \"their\", \"theirs\", \"the\", \"has\", \"that\", \"all\", \"he\", \"she\", \"they\", \"them\",\n",
    "                         \"you\", \"yours\", \"was\", \"her\" ]\n",
    "    \n",
    "    for mot in corpus_mots_full:\n",
    "        if mot not in mots_a_supprimer and len(mot) >= 3:\n",
    "            corpus_mots.append(mot)\n",
    "    \n",
    "    print(\"longueur avant nettoyage:\", len(corpus_mots_full)) \n",
    "    print(\"longueur après nettoyage:\", len(corpus_mots)) \n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # écrire le compteur\n",
    "    \n",
    "    # 1) on crée notre variable contenant les résultats.\n",
    "    # ici, on va faire un dictionnaire qui associe nos mots (en \"clé\") au nombre de fois qu'elles sont utilisées.\n",
    "    compteur = {}\n",
    "    \n",
    "    # 2) on génère ensuite une liste dédoublonnée des mots du texte. \n",
    "    # pour ça il y a plusieurs méthodes, plus ou moins flemmardes.\n",
    "    \n",
    "    # on dédoublonne `corpus_mots`\n",
    "    mots_uniques = set(corpus_mots)\n",
    "    print(\"nombre de mots uniques:\", len(mots_uniques))\n",
    "    \n",
    "    # 3) on complète le compteur \n",
    "    \n",
    "    # on initie le compteur en associant chaque mot unique à 0. \n",
    "    # 0 sera ensuite remplacé par le nb d'occurrences du mot.\n",
    "    for mot in mots_uniques:\n",
    "        compteur[mot] = 0\n",
    "    # ensuite, on boucle sur `corpus_mots` (ensemble des mots, avec doublons), et à chaque fois\n",
    "    # qu'un mot apparaît, on augmente son décompte de 1.\n",
    "    for mot in corpus_mots:\n",
    "        compteur[mot] += 1\n",
    "    \n",
    "    # on identifie tous les mots les plus utilisés\n",
    "    maxuse = max(list(compteur.values()))  # valeur maximale de notre dictionnaire\n",
    "    mots_frequents = []\n",
    "    for key, val in compteur.items():\n",
    "        if val == maxuse:\n",
    "            mots_frequents.append(key)\n",
    "    print(\"mots les plus fréquemment utilisés:\", mots_frequents)\n",
    "    \n",
    "    somme_utilisations = 0\n",
    "    nb_mots_uniques = len(mots_uniques)\n",
    "    for val in compteur.values():\n",
    "        somme_utilisations += val\n",
    "    print(\"nombre moyen d'occurrences d'un mot: \", somme_utilisations / nb_mots_uniques)\n",
    "\n",
    "\n",
    "count_words(mrs_dalloway)\n",
    "print(\"\")\n",
    "count_words(the_waves)\n",
    "print(\"\")\n",
    "count_words(lighthouse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
